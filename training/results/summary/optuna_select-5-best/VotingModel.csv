dataset,model,best_params,final_test_results-accuracy,final_test_results-f1_score,final_test_results-training_time
impute_standard,VotingModel,"{""lr_C"": 63.45427798070067, ""lr_max_iter"": 700, ""gnb_var_smoothing"": 3.667961481930031e-08, ""qda_reg_param"": 0.2108341416890651, ""voting_weight_lr"": 0.8018088073359225, ""voting_weight_gnb"": 1.045673855105345, ""voting_weight_qda"": 1.708446998426489, ""voting"": ""soft""}",0.8430913348946136,0.7581227436823105,0.01305389404296875
mean_standard,VotingModel,"{""lr_C"": 0.3989358142142236, ""lr_max_iter"": 700, ""gnb_var_smoothing"": 1.082868086946494e-05, ""qda_reg_param"": 0.5547359521594182, ""voting_weight_lr"": 1.891086906378383, ""voting_weight_gnb"": 1.8462294018449077, ""voting_weight_qda"": 1.6851835335651428, ""voting"": ""soft""}",0.8313817330210773,0.7428571428571429,0.012638092041015625
